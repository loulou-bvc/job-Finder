from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time

# ğŸ“Œ Configuration du WebDriver
CHROMEDRIVER_PATH = "/opt/homebrew/bin/chromedriver"  
service = Service(CHROMEDRIVER_PATH)
options = webdriver.ChromeOptions()

# âœ… Mode navigation privÃ©e
options.add_argument("--incognito")

# âœ… Modifier lâ€™User-Agent pour Ãªtre moins dÃ©tectable
options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36")

# âœ… EmpÃªcher Selenium dâ€™Ãªtre dÃ©tectÃ©
options.add_argument("--disable-blink-features=AutomationControlled")
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option("useAutomationExtension", False)

# âœ… Supprimer cookies et sessions pour Ã©viter le tracking
options.add_argument("--disable-gpu") 
options.add_argument("--start-maximized")
options.add_argument("--disable-extensions")
options.add_argument("--disable-popup-blocking")

driver = webdriver.Chrome(service=service, options=options)

# â›” Supprimer les traces Selenium
driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

# âœ… Charger la page d'accueil d'Indeed pour Ã©viter `data:`
driver.get("https://fr.indeed.com/")

# âœ… Attendre que la page soit bien chargÃ©e
WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

# ğŸš€ Nettoyer cookies et stockage pour Ã©viter d'Ãªtre dÃ©tectÃ© comme robot
try:
    driver.delete_all_cookies()
    driver.execute_script("window.localStorage.clear(); window.sessionStorage.clear();")
except:
    print("âš ï¸ Impossible d'effacer localStorage/sessionStorage (possible restriction).")

# ğŸ Saisie des paramÃ¨tres
base_url = input("ğŸ”— URL Indeed de la recherche : ").strip()
if "&start=" not in base_url:
    base_url += "&start="  # Ajoute le paramÃ¨tre de pagination si absent

# ğŸ“Œ PrÃ©remplissage des colonnes
print("\nğŸ’¡ Laissez vide pour ne pas prÃ©remplir une colonne.")
default_type_contrat = input("ğŸ“Œ Type de contrat : ").strip()
default_ville = input("ğŸŒ Ville : ").strip()
default_departement = input("ğŸ“ DÃ©partement : ").strip()
default_domaine = input("ğŸ’¼ Domaine : ").strip()

# ğŸ“„ Nombre de pages Ã  scraper
while True:
    try:
        nombre_pages = int(input("ğŸ“‘ Combien de pages souhaitez-vous scraper ? (10 offres par page) : "))
        break
    except ValueError:
        print("âŒ Veuillez entrer un nombre valide.")

# ğŸ“ Fonction pour extraire les offres

# ğŸ“ Fonction pour extraire les offres
def extract_jobs(page):
    # VÃ©rifier si "&start=" est dÃ©jÃ  prÃ©sent dans l'URL
    if "&start=" in base_url:
        base_cleaned = base_url.split("&start=")[0]  # Supprime la pagination existante
    else:
        base_cleaned = base_url

    url = f"{base_cleaned}&start={page * 10}"  # ğŸ”¹ GÃ©nÃ©rer correctement l'URL
    print(f"ğŸ“¡ Scraping page {page + 1}/{nombre_pages}...")
    print(f"ğŸ”— URL : {url}")

    driver.get(url)

    # â¸ï¸ Pause pour CAPTCHA
    input("âš ï¸ Si un CAPTCHA apparaÃ®t, rÃ©sous-le puis appuie sur [ENTRÃ‰E] pour continuer...")

    print("âœ… CAPTCHA validÃ©. DÃ©but du scraping...")
    time.sleep(3)  # Pause pour laisser la page charger
    
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "job_seen_beacon"))
        )
    except:
        print("âš ï¸ Aucune offre trouvÃ©e sur cette page.")
        return []

    # ğŸ“Œ RÃ©cupÃ©ration des offres
    job_cards = driver.find_elements(By.CLASS_NAME, "job_seen_beacon")
    
    if not job_cards:
        job_cards = driver.find_elements(By.CLASS_NAME, "css-1ac2h1w")  # â¬…ï¸ Test avec une autre classe
        print("ğŸ” Essai avec un autre sÃ©lecteur.")

    jobs = []
    
    for card in job_cards:
        try:
            title_element = card.find_element(By.CLASS_NAME, "jobTitle")
            url_element = card.find_element(By.TAG_NAME, "a")

            try:
                company_element = card.find_element(By.CLASS_NAME, "css-1h7lukg")
                company = company_element.text.strip()
            except:
                company = "Non prÃ©cisÃ©"

            try:
                location_element = card.find_element(By.CLASS_NAME, "css-1restlb")
                location = location_element.text.strip()
            except:
                location = default_ville if default_ville else "Non prÃ©cisÃ©"

            try:
                type_contrat_element = card.find_element(By.CLASS_NAME, "css-18z4q2i")
                type_contrat = type_contrat_element.text.strip()
            except:
                type_contrat = default_type_contrat if default_type_contrat else "Non prÃ©cisÃ©"

            title = title_element.text.strip() if title_element else "Non prÃ©cisÃ©"
            job_url = "https://fr.indeed.com" + url_element.get_attribute("href") if url_element else "Non disponible"

            departement = default_departement if default_departement else (
                location.split(" ")[-1] if location.split(" ")[-1].isdigit() else "Non prÃ©cisÃ©"
            )

            domaine = default_domaine if default_domaine else "Non prÃ©cisÃ©"

            job_data = [company, title, job_url, location, departement, domaine, type_contrat]
            jobs.append(job_data)

        except Exception as e:
            print(f"âš ï¸ Erreur sur une offre : {e}")

    return jobs

# ğŸ“Œ Lancer le scraping sur plusieurs pages
all_jobs = []
for page in range(nombre_pages):
    all_jobs.extend(extract_jobs(page))

# ğŸ“‚ Sauvegarde des rÃ©sultats
df = pd.DataFrame(all_jobs, columns=['Entreprise', 'Titre', 'Lien', 'Ville', 'DÃ©partement', 'Domaine', 'Type de Contrat'])
df.to_csv("indeed_offres.csv", index=False, encoding="utf-8")

print(f"\nâœ… {len(all_jobs)} offres ont Ã©tÃ© enregistrÃ©es.")
driver.quit()
